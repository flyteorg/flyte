// @generated by protoc-gen-es v1.7.2 with parameter "target=ts"
// @generated from file flyteidl/plugins/spark.proto (package flyteidl.plugins, syntax proto3)
/* eslint-disable */
// @ts-nocheck

import type { BinaryReadOptions, FieldList, JsonReadOptions, JsonValue, PartialMessage, PlainMessage } from "@bufbuild/protobuf";
import { Message, proto3, Struct } from "@bufbuild/protobuf";

/**
 * @generated from message flyteidl.plugins.SparkApplication
 */
export class SparkApplication extends Message<SparkApplication> {
  constructor(data?: PartialMessage<SparkApplication>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "flyteidl.plugins.SparkApplication";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): SparkApplication {
    return new SparkApplication().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): SparkApplication {
    return new SparkApplication().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): SparkApplication {
    return new SparkApplication().fromJsonString(jsonString, options);
  }

  static equals(a: SparkApplication | PlainMessage<SparkApplication> | undefined, b: SparkApplication | PlainMessage<SparkApplication> | undefined): boolean {
    return proto3.util.equals(SparkApplication, a, b);
  }
}

/**
 * @generated from enum flyteidl.plugins.SparkApplication.Type
 */
export enum SparkApplication_Type {
  /**
   * @generated from enum value: PYTHON = 0;
   */
  PYTHON = 0,

  /**
   * @generated from enum value: JAVA = 1;
   */
  JAVA = 1,

  /**
   * @generated from enum value: SCALA = 2;
   */
  SCALA = 2,

  /**
   * @generated from enum value: R = 3;
   */
  R = 3,
}
// Retrieve enum metadata with: proto3.getEnumType(SparkApplication_Type)
proto3.util.setEnumType(SparkApplication_Type, "flyteidl.plugins.SparkApplication.Type", [
  { no: 0, name: "PYTHON" },
  { no: 1, name: "JAVA" },
  { no: 2, name: "SCALA" },
  { no: 3, name: "R" },
]);

/**
 * Custom Proto for Spark Plugin.
 *
 * @generated from message flyteidl.plugins.SparkJob
 */
export class SparkJob extends Message<SparkJob> {
  /**
   * @generated from field: flyteidl.plugins.SparkApplication.Type applicationType = 1;
   */
  applicationType = SparkApplication_Type.PYTHON;

  /**
   * @generated from field: string mainApplicationFile = 2;
   */
  mainApplicationFile = "";

  /**
   * @generated from field: string mainClass = 3;
   */
  mainClass = "";

  /**
   * @generated from field: map<string, string> sparkConf = 4;
   */
  sparkConf: { [key: string]: string } = {};

  /**
   * @generated from field: map<string, string> hadoopConf = 5;
   */
  hadoopConf: { [key: string]: string } = {};

  /**
   * Executor path for Python jobs.
   *
   * @generated from field: string executorPath = 6;
   */
  executorPath = "";

  /**
   * Databricks job configuration.
   * Config structure can be found here. https://docs.databricks.com/dev-tools/api/2.0/jobs.html#request-structure.
   *
   * @generated from field: google.protobuf.Struct databricksConf = 7;
   */
  databricksConf?: Struct;

  /**
   * Databricks access token. https://docs.databricks.com/dev-tools/api/latest/authentication.html
   * This token can be set in either flytepropeller or flytekit.
   *
   * @generated from field: string databricksToken = 8;
   */
  databricksToken = "";

  /**
   * Domain name of your deployment. Use the form <account>.cloud.databricks.com.
   * This instance name can be set in either flytepropeller or flytekit.
   *
   * @generated from field: string databricksInstance = 9;
   */
  databricksInstance = "";

  constructor(data?: PartialMessage<SparkJob>) {
    super();
    proto3.util.initPartial(data, this);
  }

  static readonly runtime: typeof proto3 = proto3;
  static readonly typeName = "flyteidl.plugins.SparkJob";
  static readonly fields: FieldList = proto3.util.newFieldList(() => [
    { no: 1, name: "applicationType", kind: "enum", T: proto3.getEnumType(SparkApplication_Type) },
    { no: 2, name: "mainApplicationFile", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 3, name: "mainClass", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 4, name: "sparkConf", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
    { no: 5, name: "hadoopConf", kind: "map", K: 9 /* ScalarType.STRING */, V: {kind: "scalar", T: 9 /* ScalarType.STRING */} },
    { no: 6, name: "executorPath", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 7, name: "databricksConf", kind: "message", T: Struct },
    { no: 8, name: "databricksToken", kind: "scalar", T: 9 /* ScalarType.STRING */ },
    { no: 9, name: "databricksInstance", kind: "scalar", T: 9 /* ScalarType.STRING */ },
  ]);

  static fromBinary(bytes: Uint8Array, options?: Partial<BinaryReadOptions>): SparkJob {
    return new SparkJob().fromBinary(bytes, options);
  }

  static fromJson(jsonValue: JsonValue, options?: Partial<JsonReadOptions>): SparkJob {
    return new SparkJob().fromJson(jsonValue, options);
  }

  static fromJsonString(jsonString: string, options?: Partial<JsonReadOptions>): SparkJob {
    return new SparkJob().fromJsonString(jsonString, options);
  }

  static equals(a: SparkJob | PlainMessage<SparkJob> | undefined, b: SparkJob | PlainMessage<SparkJob> | undefined): boolean {
    return proto3.util.equals(SparkJob, a, b);
  }
}

