# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: flyteidl2/plugins/kubeflow/pytorch.proto
"""Generated protocol buffer code."""
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import symbol_database as _symbol_database
from google.protobuf.internal import builder as _builder
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from flyteidl2.core import tasks_pb2 as flyteidl2_dot_core_dot_tasks__pb2
from flyteidl2.plugins import common_pb2 as flyteidl2_dot_plugins_dot_common__pb2
from flyteidl2.plugins.kubeflow import common_pb2 as flyteidl2_dot_plugins_dot_kubeflow_dot_common__pb2


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n(flyteidl2/plugins/kubeflow/pytorch.proto\x12\x1a\x66lyteidl2.plugins.kubeflow\x1a\x1a\x66lyteidl2/core/tasks.proto\x1a\x1e\x66lyteidl2/plugins/common.proto\x1a\'flyteidl2/plugins/kubeflow/common.proto\"\xc1\x01\n\rElasticConfig\x12!\n\x0crdzv_backend\x18\x01 \x01(\tR\x0brdzvBackend\x12!\n\x0cmin_replicas\x18\x02 \x01(\x05R\x0bminReplicas\x12!\n\x0cmax_replicas\x18\x03 \x01(\x05R\x0bmaxReplicas\x12$\n\x0enproc_per_node\x18\x04 \x01(\x05R\x0cnprocPerNode\x12!\n\x0cmax_restarts\x18\x05 \x01(\x05R\x0bmaxRestarts\"\x90\x03\n\x1e\x44istributedPyTorchTrainingTask\x12j\n\x0fworker_replicas\x18\x01 \x01(\x0b\x32\x41.flyteidl2.plugins.kubeflow.DistributedPyTorchTrainingReplicaSpecR\x0eworkerReplicas\x12j\n\x0fmaster_replicas\x18\x02 \x01(\x0b\x32\x41.flyteidl2.plugins.kubeflow.DistributedPyTorchTrainingReplicaSpecR\x0emasterReplicas\x12\x44\n\nrun_policy\x18\x03 \x01(\x0b\x32%.flyteidl2.plugins.kubeflow.RunPolicyR\trunPolicy\x12P\n\x0e\x65lastic_config\x18\x04 \x01(\x0b\x32).flyteidl2.plugins.kubeflow.ElasticConfigR\relasticConfig\"\xa9\x02\n%DistributedPyTorchTrainingReplicaSpec\x12\x1e\n\x08replicas\x18\x01 \x01(\x05\x42\x02\x18\x01R\x08replicas\x12\x18\n\x05image\x18\x02 \x01(\tB\x02\x18\x01R\x05image\x12;\n\tresources\x18\x03 \x01(\x0b\x32\x19.flyteidl2.core.ResourcesB\x02\x18\x01R\tresources\x12K\n\x0erestart_policy\x18\x04 \x01(\x0e\x32 .flyteidl2.plugins.RestartPolicyB\x02\x18\x01R\rrestartPolicy\x12<\n\x06\x63ommon\x18\x05 \x01(\x0b\x32$.flyteidl2.plugins.CommonReplicaSpecR\x06\x63ommonB\xfa\x01\n\x1e\x63om.flyteidl2.plugins.kubeflowB\x0cPytorchProtoH\x02P\x01Z>github.com/flyteorg/flyte/v2/gen/go/flyteidl2/plugins/kubeflow\xa2\x02\x03\x46PK\xaa\x02\x1a\x46lyteidl2.Plugins.Kubeflow\xca\x02\x1a\x46lyteidl2\\Plugins\\Kubeflow\xe2\x02&Flyteidl2\\Plugins\\Kubeflow\\GPBMetadata\xea\x02\x1c\x46lyteidl2::Plugins::Kubeflowb\x06proto3')

_globals = globals()
_builder.BuildMessageAndEnumDescriptors(DESCRIPTOR, _globals)
_builder.BuildTopDescriptorsAndMessages(DESCRIPTOR, 'flyteidl2.plugins.kubeflow.pytorch_pb2', _globals)
if _descriptor._USE_C_DESCRIPTORS == False:
  DESCRIPTOR._options = None
  DESCRIPTOR._serialized_options = b'\n\036com.flyteidl2.plugins.kubeflowB\014PytorchProtoH\002P\001Z>github.com/flyteorg/flyte/v2/gen/go/flyteidl2/plugins/kubeflow\242\002\003FPK\252\002\032Flyteidl2.Plugins.Kubeflow\312\002\032Flyteidl2\\Plugins\\Kubeflow\342\002&Flyteidl2\\Plugins\\Kubeflow\\GPBMetadata\352\002\034Flyteidl2::Plugins::Kubeflow'
  _DISTRIBUTEDPYTORCHTRAININGREPLICASPEC.fields_by_name['replicas']._options = None
  _DISTRIBUTEDPYTORCHTRAININGREPLICASPEC.fields_by_name['replicas']._serialized_options = b'\030\001'
  _DISTRIBUTEDPYTORCHTRAININGREPLICASPEC.fields_by_name['image']._options = None
  _DISTRIBUTEDPYTORCHTRAININGREPLICASPEC.fields_by_name['image']._serialized_options = b'\030\001'
  _DISTRIBUTEDPYTORCHTRAININGREPLICASPEC.fields_by_name['resources']._options = None
  _DISTRIBUTEDPYTORCHTRAININGREPLICASPEC.fields_by_name['resources']._serialized_options = b'\030\001'
  _DISTRIBUTEDPYTORCHTRAININGREPLICASPEC.fields_by_name['restart_policy']._options = None
  _DISTRIBUTEDPYTORCHTRAININGREPLICASPEC.fields_by_name['restart_policy']._serialized_options = b'\030\001'
  _globals['_ELASTICCONFIG']._serialized_start=174
  _globals['_ELASTICCONFIG']._serialized_end=367
  _globals['_DISTRIBUTEDPYTORCHTRAININGTASK']._serialized_start=370
  _globals['_DISTRIBUTEDPYTORCHTRAININGTASK']._serialized_end=770
  _globals['_DISTRIBUTEDPYTORCHTRAININGREPLICASPEC']._serialized_start=773
  _globals['_DISTRIBUTEDPYTORCHTRAININGREPLICASPEC']._serialized_end=1070
# @@protoc_insertion_point(module_scope)
