// @generated by protoc-gen-es v2.2.5 with parameter "target=ts,import_extension=.ts"
// @generated from file flyteidl2/plugins/kubeflow/pytorch.proto (package flyteidl2.plugins.kubeflow, syntax proto3)
/* eslint-disable */

import type { GenFile, GenMessage } from "@bufbuild/protobuf/codegenv1";
import { fileDesc, messageDesc } from "@bufbuild/protobuf/codegenv1";
import type { Resources } from "../../core/tasks_pb.ts";
import { file_flyteidl2_core_tasks } from "../../core/tasks_pb.ts";
import type { CommonReplicaSpec, RestartPolicy } from "../common_pb.ts";
import { file_flyteidl2_plugins_common } from "../common_pb.ts";
import type { RunPolicy } from "./common_pb.ts";
import { file_flyteidl2_plugins_kubeflow_common } from "./common_pb.ts";
import type { Message } from "@bufbuild/protobuf";

/**
 * Describes the file flyteidl2/plugins/kubeflow/pytorch.proto.
 */
export const file_flyteidl2_plugins_kubeflow_pytorch: GenFile = /*@__PURE__*/
  fileDesc("CihmbHl0ZWlkbDIvcGx1Z2lucy9rdWJlZmxvdy9weXRvcmNoLnByb3RvEhpmbHl0ZWlkbDIucGx1Z2lucy5rdWJlZmxvdyJ/Cg1FbGFzdGljQ29uZmlnEhQKDHJkenZfYmFja2VuZBgBIAEoCRIUCgxtaW5fcmVwbGljYXMYAiABKAUSFAoMbWF4X3JlcGxpY2FzGAMgASgFEhYKDm5wcm9jX3Blcl9ub2RlGAQgASgFEhQKDG1heF9yZXN0YXJ0cxgFIAEoBSLWAgoeRGlzdHJpYnV0ZWRQeVRvcmNoVHJhaW5pbmdUYXNrEloKD3dvcmtlcl9yZXBsaWNhcxgBIAEoCzJBLmZseXRlaWRsMi5wbHVnaW5zLmt1YmVmbG93LkRpc3RyaWJ1dGVkUHlUb3JjaFRyYWluaW5nUmVwbGljYVNwZWMSWgoPbWFzdGVyX3JlcGxpY2FzGAIgASgLMkEuZmx5dGVpZGwyLnBsdWdpbnMua3ViZWZsb3cuRGlzdHJpYnV0ZWRQeVRvcmNoVHJhaW5pbmdSZXBsaWNhU3BlYxI5CgpydW5fcG9saWN5GAMgASgLMiUuZmx5dGVpZGwyLnBsdWdpbnMua3ViZWZsb3cuUnVuUG9saWN5EkEKDmVsYXN0aWNfY29uZmlnGAQgASgLMikuZmx5dGVpZGwyLnBsdWdpbnMua3ViZWZsb3cuRWxhc3RpY0NvbmZpZyL2AQolRGlzdHJpYnV0ZWRQeVRvcmNoVHJhaW5pbmdSZXBsaWNhU3BlYxIUCghyZXBsaWNhcxgBIAEoBUICGAESEQoFaW1hZ2UYAiABKAlCAhgBEjAKCXJlc291cmNlcxgDIAEoCzIZLmZseXRlaWRsMi5jb3JlLlJlc291cmNlc0ICGAESPAoOcmVzdGFydF9wb2xpY3kYBCABKA4yIC5mbHl0ZWlkbDIucGx1Z2lucy5SZXN0YXJ0UG9saWN5QgIYARI0CgZjb21tb24YBSABKAsyJC5mbHl0ZWlkbDIucGx1Z2lucy5Db21tb25SZXBsaWNhU3BlY0L6AQoeY29tLmZseXRlaWRsMi5wbHVnaW5zLmt1YmVmbG93QgxQeXRvcmNoUHJvdG9IAlABWj5naXRodWIuY29tL2ZseXRlb3JnL2ZseXRlL3YyL2dlbi9nby9mbHl0ZWlkbDIvcGx1Z2lucy9rdWJlZmxvd6ICA0ZQS6oCGkZseXRlaWRsMi5QbHVnaW5zLkt1YmVmbG93ygIaRmx5dGVpZGwyXFBsdWdpbnNcS3ViZWZsb3fiAiZGbHl0ZWlkbDJcUGx1Z2luc1xLdWJlZmxvd1xHUEJNZXRhZGF0YeoCHEZseXRlaWRsMjo6UGx1Z2luczo6S3ViZWZsb3diBnByb3RvMw", [file_flyteidl2_core_tasks, file_flyteidl2_plugins_common, file_flyteidl2_plugins_kubeflow_common]);

/**
 * Custom proto for torch elastic config for distributed training using
 * https://github.com/kubeflow/training-operator/blob/master/pkg/apis/kubeflow.org/v1/pytorch_types.go
 *
 * @generated from message flyteidl2.plugins.kubeflow.ElasticConfig
 */
export type ElasticConfig = Message<"flyteidl2.plugins.kubeflow.ElasticConfig"> & {
  /**
   * @generated from field: string rdzv_backend = 1;
   */
  rdzvBackend: string;

  /**
   * @generated from field: int32 min_replicas = 2;
   */
  minReplicas: number;

  /**
   * @generated from field: int32 max_replicas = 3;
   */
  maxReplicas: number;

  /**
   * @generated from field: int32 nproc_per_node = 4;
   */
  nprocPerNode: number;

  /**
   * @generated from field: int32 max_restarts = 5;
   */
  maxRestarts: number;
};

/**
 * Describes the message flyteidl2.plugins.kubeflow.ElasticConfig.
 * Use `create(ElasticConfigSchema)` to create a new message.
 */
export const ElasticConfigSchema: GenMessage<ElasticConfig> = /*@__PURE__*/
  messageDesc(file_flyteidl2_plugins_kubeflow_pytorch, 0);

/**
 * Proto for plugin that enables distributed training using https://github.com/kubeflow/pytorch-operator
 *
 * @generated from message flyteidl2.plugins.kubeflow.DistributedPyTorchTrainingTask
 */
export type DistributedPyTorchTrainingTask = Message<"flyteidl2.plugins.kubeflow.DistributedPyTorchTrainingTask"> & {
  /**
   * Worker replicas spec
   *
   * @generated from field: flyteidl2.plugins.kubeflow.DistributedPyTorchTrainingReplicaSpec worker_replicas = 1;
   */
  workerReplicas?: DistributedPyTorchTrainingReplicaSpec;

  /**
   * Master replicas spec, master replicas can only have 1 replica
   *
   * @generated from field: flyteidl2.plugins.kubeflow.DistributedPyTorchTrainingReplicaSpec master_replicas = 2;
   */
  masterReplicas?: DistributedPyTorchTrainingReplicaSpec;

  /**
   * RunPolicy encapsulates various runtime policies of the distributed training
   * job, for example how to clean up resources and how long the job can stay
   * active.
   *
   * @generated from field: flyteidl2.plugins.kubeflow.RunPolicy run_policy = 3;
   */
  runPolicy?: RunPolicy;

  /**
   * config for an elastic pytorch job
   *
   * @generated from field: flyteidl2.plugins.kubeflow.ElasticConfig elastic_config = 4;
   */
  elasticConfig?: ElasticConfig;
};

/**
 * Describes the message flyteidl2.plugins.kubeflow.DistributedPyTorchTrainingTask.
 * Use `create(DistributedPyTorchTrainingTaskSchema)` to create a new message.
 */
export const DistributedPyTorchTrainingTaskSchema: GenMessage<DistributedPyTorchTrainingTask> = /*@__PURE__*/
  messageDesc(file_flyteidl2_plugins_kubeflow_pytorch, 1);

/**
 * @generated from message flyteidl2.plugins.kubeflow.DistributedPyTorchTrainingReplicaSpec
 */
export type DistributedPyTorchTrainingReplicaSpec = Message<"flyteidl2.plugins.kubeflow.DistributedPyTorchTrainingReplicaSpec"> & {
  /**
   * 1~4 deprecated. Use common instead.
   * Number of replicas
   *
   * @generated from field: int32 replicas = 1 [deprecated = true];
   * @deprecated
   */
  replicas: number;

  /**
   * Image used for the replica group
   *
   * @generated from field: string image = 2 [deprecated = true];
   * @deprecated
   */
  image: string;

  /**
   * Resources required for the replica group
   *
   * @generated from field: flyteidl2.core.Resources resources = 3 [deprecated = true];
   * @deprecated
   */
  resources?: Resources;

  /**
   * Restart policy determines whether pods will be restarted when they exit
   *
   * @generated from field: flyteidl2.plugins.RestartPolicy restart_policy = 4 [deprecated = true];
   * @deprecated
   */
  restartPolicy: RestartPolicy;

  /**
   * The common replica spec
   *
   * @generated from field: flyteidl2.plugins.CommonReplicaSpec common = 5;
   */
  common?: CommonReplicaSpec;
};

/**
 * Describes the message flyteidl2.plugins.kubeflow.DistributedPyTorchTrainingReplicaSpec.
 * Use `create(DistributedPyTorchTrainingReplicaSpecSchema)` to create a new message.
 */
export const DistributedPyTorchTrainingReplicaSpecSchema: GenMessage<DistributedPyTorchTrainingReplicaSpec> = /*@__PURE__*/
  messageDesc(file_flyteidl2_plugins_kubeflow_pytorch, 2);

